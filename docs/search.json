[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site shares the code and details about a project investigating r/foodstamps.\nIt was created by Stuart Deets, a Ph.D. student at the University of Minnesota, as part of a seminar in computational methods to study texts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Topic Modeling & r/foodstamps",
    "section": "",
    "text": "SNAP stands for Supplemental Nutrition Assistance Program.\n\nAdministered state by state; different states have different rules. It is funded by the US Department of Agriculture.\nFormerly known as food stamps, today typically an electronic debit-card style, while it used to be true that it was literally distributed as stamps\nGoal: reduce hunger\n\nIt is a means-tested program, which means that if an individual makes above a certain threshold of income, they will receive reduced benefits, or are even no longer eligible to receive any money. Because of the means-tested nature of the benefit, this means that there are significant administrative costs on governments at nearly 15.8 cents of administrative costs per dollar of benefits disbursed. Moreover, these costs are also borne by program participants, who must navigate and deal with the demands of eligibility workers and policymakers.\nWhile the goal of the program is to smooth the income of individuals and families, because people’s income varies over time, there are significant information and administrative problems associated with the program. For instance, consider someone whose job lays them off in March but is rehired by a new business in May, so they are out of work for about 2 1/2 months. During those months, they do not have any income. Because it takes time for the paperwork to process, they may not receive benefits for a few weeks or even a few months after applying to SNAP. Yet, because they are hired in May, they are then no longer eligible for benefits. Depending on the state, they likely have to fill out paperwork each month verifying their income levels. Ideally, this would solve the problem. However, some SNAP participants experience painful “clawbacks” in which the government takes money back from welfare recepients because the government overpaid the benefits.\nMoreover, because of the complicated nature of the program and the eligibility requirements, which include meeting with caseworkers and submitting paperwork, there are high information and administrative issues on the governmental side, as well as adverse outcomes. In California, economic analyses indicate that even when SNAP participants are removed from the program for non-compliance, they often return as soon as the next month as their “true” eligibility has not changed. This highlights the “administrative burden” that SNAP participants face in the program.\nAdditionally, SNAP benefits are disbursed on a specific day of the month in different states.\n\n\n\nAdministrative burden is the idea that the burdens that citizens face can influence policy outcomes. Herd & Moynihan (2018) have identified that there are three major types of burdens: - Learning costs. These are incurred by users of government programs as they have to learn how to navigate the program. It is sometimes called an “evaluation” cost, or even a “search” cost. - Psychological costs. These are borne by citizens from stigma, loss of autonomy, or frustration, and also general stress at dealing with obtuse government systems. - Compliance costs. These are the costs that it takes to comply with a particular rule or regulation, including the travel costs, but is more generally understood as the “provision of information or documentation to document standing.” Each of these different costs is distinct, but there are also overlaps. Addressing administrative burden has become an initative of the Biden administration and the Office of Management and Budget, and the Office of Information & Regulatory Affairs.\n\n\n\n\nAdministrative burden is present in the program. Means-testing exacerbates the issue\nScholars are studying SNAP for its administrative burden\nSNAP benefits are disbursed on specific days, see link for the full list state by state"
  },
  {
    "objectID": "index.html#what-is-food-stampssnap",
    "href": "index.html#what-is-food-stampssnap",
    "title": "Topic Modeling & r/foodstamps",
    "section": "",
    "text": "SNAP stands for Supplemental Nutrition Assistance Program.\n\nAdministered state by state; different states have different rules. It is funded by the US Department of Agriculture.\nFormerly known as food stamps, today typically an electronic debit-card style, while it used to be true that it was literally distributed as stamps\nGoal: reduce hunger\n\nIt is a means-tested program, which means that if an individual makes above a certain threshold of income, they will receive reduced benefits, or are even no longer eligible to receive any money. Because of the means-tested nature of the benefit, this means that there are significant administrative costs on governments at nearly 15.8 cents of administrative costs per dollar of benefits disbursed. Moreover, these costs are also borne by program participants, who must navigate and deal with the demands of eligibility workers and policymakers.\nWhile the goal of the program is to smooth the income of individuals and families, because people’s income varies over time, there are significant information and administrative problems associated with the program. For instance, consider someone whose job lays them off in March but is rehired by a new business in May, so they are out of work for about 2 1/2 months. During those months, they do not have any income. Because it takes time for the paperwork to process, they may not receive benefits for a few weeks or even a few months after applying to SNAP. Yet, because they are hired in May, they are then no longer eligible for benefits. Depending on the state, they likely have to fill out paperwork each month verifying their income levels. Ideally, this would solve the problem. However, some SNAP participants experience painful “clawbacks” in which the government takes money back from welfare recepients because the government overpaid the benefits.\nMoreover, because of the complicated nature of the program and the eligibility requirements, which include meeting with caseworkers and submitting paperwork, there are high information and administrative issues on the governmental side, as well as adverse outcomes. In California, economic analyses indicate that even when SNAP participants are removed from the program for non-compliance, they often return as soon as the next month as their “true” eligibility has not changed. This highlights the “administrative burden” that SNAP participants face in the program.\nAdditionally, SNAP benefits are disbursed on a specific day of the month in different states."
  },
  {
    "objectID": "index.html#administrative-burden",
    "href": "index.html#administrative-burden",
    "title": "Topic Modeling & r/foodstamps",
    "section": "",
    "text": "Administrative burden is the idea that the burdens that citizens face can influence policy outcomes. Herd & Moynihan (2018) have identified that there are three major types of burdens: - Learning costs. These are incurred by users of government programs as they have to learn how to navigate the program. It is sometimes called an “evaluation” cost, or even a “search” cost. - Psychological costs. These are borne by citizens from stigma, loss of autonomy, or frustration, and also general stress at dealing with obtuse government systems. - Compliance costs. These are the costs that it takes to comply with a particular rule or regulation, including the travel costs, but is more generally understood as the “provision of information or documentation to document standing.” Each of these different costs is distinct, but there are also overlaps. Addressing administrative burden has become an initative of the Biden administration and the Office of Management and Budget, and the Office of Information & Regulatory Affairs."
  },
  {
    "objectID": "index.html#takeaways-from-snap-program-design",
    "href": "index.html#takeaways-from-snap-program-design",
    "title": "Topic Modeling & r/foodstamps",
    "section": "",
    "text": "Administrative burden is present in the program. Means-testing exacerbates the issue\nScholars are studying SNAP for its administrative burden\nSNAP benefits are disbursed on specific days, see link for the full list state by state"
  },
  {
    "objectID": "index.html#why-is-this-of-interest-to-technical-communicators",
    "href": "index.html#why-is-this-of-interest-to-technical-communicators",
    "title": "Topic Modeling & r/foodstamps",
    "section": "Why is this of interest to technical communicators?",
    "text": "Why is this of interest to technical communicators?\nTechnical communicators are interested in communicating around complex, technical documents and information. And they are also committed in many ways to social justice and to improving outcomes in equitable ways. If technical communicators could improve the way that governments communicate to SNAP recipients, that would be a good thing!\nThis project is informed by the ongoing conversation within the discipline around Tactical Technical Communcation (TTC). TTC is differentiated within this area of inquiry from “strategic” communciations, drawing on de Certeau’s distinction between the strategic and the tactical:\n\n\n\n\n\n\nImportant\n\n\n\nA tactic differs from a strategy in that it lacks a place or “property,” relying instead on action in the “cracks” between “proprietary powers”: “It poaches in them. It creates surprises in them. It can be where it is least expected. It is a guileful ruse. In short, a tactic is an art of the weak” (de Certeau, 1984, p. 37).\n\n\nImportant examples within technical and professional communication of scholars studying TTC include, but is not limited to: - Colton, J. S., Holmes, S., & Walwema, J. (2017). From NoobGuides to #OpKKK: Ethics of Anonymous’ Tactical Technical Communication. Technical Communication Quarterly, 26(1), 59–75. https://doi.org/10.1080/10572252.2016.1257743\n\nEdenfield, A. C. (2019). Queering consent: Design and sexual consent messaging. Communication Design Quarterly, 7(2), 50–63. https://doi.org/10.1145/3358931.3358938\nEdenfield, A. C. (2021). Managing Gender Care in Precarity: Trans Communities Respond to COVID-19. Journal of Business and Technical Communication, 35(1), 15–21. https://doi.org/10.1177/1050651920958504\nEdenfield, A. C., Holmes, S., & Colton, J. S. (2019). Queering Tactical Technical Communication: DIY HRT. Technical Communication Quarterly, 28(3), 177–191. https://doi.org/10.1080/10572252.2019.1607906\nHolladay, D. (2017). Classified Conversations: Psychiatry and Tactical Technical Communication in Online Spaces. Technical Communication Quarterly, 26(1), 8–24. https://doi.org/10.1080/10572252.2016.1257744\nKimball, M. A. (2006). Cars, Culture, and Tactical Technical Communication. Technical Communication Quarterly, 15(1), 67–86. https://doi.org/10.1207/s15427625tcq1501_6\nKimball, M. A. (2017). Tactical Technical Communication. Technical Communication Quarterly, 26(1), 1–7. https://doi.org/10.1080/10572252.2017.1259428\nPeter, H. A. S.-S. (2017). “Make a Bomb in the Kitchen of Your Mom”: Jihadist Tactical Technical Communication and the Everyday Practice of Cooking. Technical Communication Quarterly, 26(1), 76–91. https://doi.org/10.1080/10572252.2016.1275862\n\nThese articles, starting with Kimball’s “Cars, Culture, and Tactical Technical Communication,” discuss the ways that users seek to navigate power systems and find dynamic ways to generate solutions. For instance, in the original Kimball article, the main focus is users who are interested in fixing their cars themselves. Today, however, in conjunction with the social justice turn, scholars have pointed out the ways that queer people use tactical technical communication, or even how users repond to psychiatric diagnoses.\nArguably, we can see tactical technical communication happening in r/foodstamps, in which users practice “an art of the weak” as they navigate between cracks in strategic communication by the government or non-profits. While individual users may be assigned “flairs” or special tags in their profile that indicate when they comment about their role, such as “SNAP policy expert” or “SNAP eligibility expert - STATE”, it is still the case that users are turning to the subreddit in order to navigate between the cracks of more formalized systems.\nAn important question that remains, then, is how to characterize the nature of the TTC that is happening on r/foodstamps. Importantly, all of the previous approaches to understanding tactical technical communication have taken a much more qualitative approach, characterized by interviews and close reading. Therefore, as this Quarto website uses a computational approach, it offers a novel way to think about and analyze tactical technical communication."
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Topic Modeling & r/foodstamps",
    "section": "Research questions",
    "text": "Research questions\n\nDoes the composition of topics change over time for posts about California?\nHow do they do so?\nDo the topics vary based on the day of the month and the number of days it has been since SNAP benefits were disbursed?\n\nThe answers to these questions are important, because if it is true that the composition of topics varies based on the number of days it has been since SNAP benefits were disbursed, that could indicate that users were turning to the subreddit as they experience problems with their disbursement or eligibility.\nConsider for example one potential story of a poster, who expected to receive their EBT money on the 3rd of the month. It is now the 5th, so he posts on the subreddit asking a few questions hoping to receive an answer. On the other hand, consider another potential story about a poster who is asking about the backlog of cases. Often, caseworkers fall behind in their verification checks, sometimes up to 120 days behind. A user who was caught up in the backlog could post at any time of the month to seek out additional information about when their application was likely to be processed, and may not be more likely to post at a particular time of the month.\n\nAbout topic modeling\nConveniently, topic modeling can pick up on the different types of words that people use, and sort them into probabilities. After looking at a lot of posts, a topic model can suggest that the two stories I laid out are likely to belong to different topics. Then, if the proportion of topics is very different on the 10th day of the month than, say, the 20th, that could indicate that when users experience specific problems, they turn to the subreddit.\nIt is important to point out the limitations of this approach, as well. A topic model can do a good job of so-called “distant reading,” but it does not do a good job of picking up ideas and concepts that a close reading approach done by a human is able to understand.\n\n\n\n\n\n\nNote\n\n\n\nTopic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.\n\n\nWhat a topic model does is understand the words that are in a document, in our case, a post, using a “bag of words” approach that is more able to understand how words are “dependent on the broader context that they are used in natural language”. Then, the topic model asks the human, “how many categories or topics do you think that there are?” After making this choice, the model then figures out which words are likely to appear within the same document. It’s then likely that, for documents where there is some overlap in terms of the words that they use, are more likely to be similar than documents that don’t use the same words. This is expressed as a “probability” that a particular document belongs in a particular category. Taking the top probability for each document, we can then graph how the aggregation of these probabilities changes over time, based on how many days it has been since SNAP began to disburse.\nWhat this approach is not is it is not “the truth” or some other perfect representation of reality. It is a flawed computer program designed by a flawed human, so it should not be taken as an ideal representation of r/foodstamps. It would be perfectly possible to look at 8000 posts and do a qualitative analysis of these posts, sorting them, tagging them, and categorizing them by hand. This would be an equally reasonable and perhaps more interesting and more valid way of approaching this data.\nHowever, given the size of the data, it’s much quicker to run a topic model than it is to develop a qualitative coding schema, so this is a reasonable first pass at this, and can allow for the generation of insights that can be developed further through interviews and more qualitative approaches."
  },
  {
    "objectID": "index.html#finding-k",
    "href": "index.html#finding-k",
    "title": "Topic Modeling & r/foodstamps",
    "section": "Finding K",
    "text": "Finding K\n\n# topic topic modeling \n## searching for K\nK &lt;- c(5,10,15,20,25) # this is what allows us to vary the # of topics.\n# Running the search (computationally intense, takes a long time)\nfit &lt;- searchK(dfm_stm$documents, dfm_stm$vocab, K = K, verbose = TRUE)\n\n\nMaking some plots that will help us find an interesting K\nWell, that took a long time, and I’ve hidden the output of the code from the Quarto website because it’s just too long. If we do some plots and graphs, that will help us visualize the output from K.\n\n# Create graph\nplot &lt;- data.frame(\"K\" = K, \n                   \"Coherence\" = unlist(fit$results$semcoh),\n                   \"Exclusivity\" = unlist(fit$results$exclus))\n\nbig_plot &lt;- plot # Saving this plot for later\n\n# Reshape to long format\nlibrary(\"reshape2\")\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\nplot &lt;- melt(plot, id=c(\"K\"))\n#Plot result\nlibrary(\"ggplot2\")\nggplot(plot, aes(K, value, color = variable)) +\n  geom_line(size = 1.5, show.legend = FALSE) +\n  facet_wrap(~variable,scales = \"free_y\") +\n  labs(x = \"Number of topics K\",\n       title = \"Statistical fit of models with different K\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "index.html#picking-a-k-number",
    "href": "index.html#picking-a-k-number",
    "title": "Topic Modeling & r/foodstamps",
    "section": "Picking a K number",
    "text": "Picking a K number\nBased on what we can see here, it seems like a good K number could be around 15. That would be a nice balance between coherence and exclusivity, two measures of a good K."
  },
  {
    "objectID": "index.html#plotting-the-topic-model",
    "href": "index.html#plotting-the-topic-model",
    "title": "Topic Modeling & r/foodstamps",
    "section": "Plotting the topic model",
    "text": "Plotting the topic model\nThis basic plot brings up the top words that are a part of each topic. It’s not very cool or interesting looking, although it has a lot of information contained within it. Ultimately, it’s not what we are interested in but it’s still important to understand.\n\nplot(model_1)"
  },
  {
    "objectID": "index.html#outputting-some-basic-charts",
    "href": "index.html#outputting-some-basic-charts",
    "title": "Topic Modeling & r/foodstamps",
    "section": "Outputting some basic charts",
    "text": "Outputting some basic charts\nNow we will try and split out some basic charts. They won’t look good, and they won’t get the job done. But they will be charts, and they will exist.\n\n# not beautiful plots of day of the month and topic\n\nplot(effect, \"day\", method = \"continuous\", topics = c(1:15), model = model_1)\n\n\n\n# Display the X-axis with the specified order\n\nLet’s add some stuff like labels to this.\n\nlibrary(ggplot2) \n\nplot(effect, \"day\", method = \"continuous\", topics = c(1:15), model =\nmodel_1, xlab = \"Day of the month\", ylab = \"Expected topic proportion\",\nprintlegend = F)"
  },
  {
    "objectID": "index.html#saving-top-features",
    "href": "index.html#saving-top-features",
    "title": "Topic Modeling & r/foodstamps",
    "section": "Saving top features",
    "text": "Saving top features\nIn case we later want to return and do some qualitative analysis and thinking about the words, lets save the outputs of our model in a .csv file. We’ll focus on FREX words, since those are typically considered to be a nice balance between exclusivity and coherence within a topic.\n\n#Save top 20 features across topics and forms of weighting \nlabels_1 &lt;- labelTopics(model_1, n=15) \n\n#only keep FREX weighting \ntopwords_1 &lt;- data.frame(\"features\" =t(labels_1$frex)) \n\n#assign topic number as column name \ncolnames(topwords_1) &lt;- paste(\"Topics\", c(1:15)) \n\n#Return the result \ntopwords_1[1:15] \n\n       Topics 1        Topics 2 Topics 3    Topics 4  Topics 5  Topics 6\n1          bank            form     says         new    stamps    cancel\n2     documents recertification approval          la      food    closed\n3  verification             sar approved        move     house    relief\n4          told             due     info     process    states      case\n5        submit      employment  website       start   savings      file\n6      accounts        services  checked     medical     place  calworks\n7         fraud          social   status application insurance     filed\n8    statements          submit      btw       state      best   hearing\n9            äù            guys    fresh        snap     state     claim\n10    submitted      work-study february       share    friend  counties\n11         okay            send     mean      system       ssi    appeal\n12       saving     benefitscal  account          id      dont     covid\n13    literally            fill   notice  electronic     wants happening\n14   submitting       statement   loaded    resident     years   general\n15      savings              pa confused    personal    health    filing\n       Topics 7  Topics 8     Topics 9  Topics 10 Topics 11  Topics 12\n1  unemployment       irt          pin       call allotment        aid\n2         march     limit         card    calling emergency  financial\n3            ui reporting       number  interview   getting     affect\n4         april    income        p-ebt        day     month   question\n5           may    report transactions        try   deposit    college\n6      payments    change          ebt     called        ea       year\n7           owe     gross        cards      never  november disability\n8  overissuance   changes          ago     office   maximum        don\n9         extra    summer       stolen supervisor     extra      meals\n10        count    within        enter     worker      date    whether\n11    receiving threshold       reload       site   october    student\n12          i’m    making        error      email    amount     grants\n13         july        gt     customer       days       nov     scared\n14        based      days     activate      phone      äôre      proof\n15  overpayment increased        tried   uploaded  pandemic        hit\n   Topics 13 Topics 14  Topics 15\n1      items   parents       cash\n2        buy      rent assistance\n3     amazon     apply       lose\n4        use   student        mom\n5    walmart     grant      money\n6    grocery  together unemployed\n7  groceries    paying       much\n8    balance     study    classes\n9   delivery   qualify      cover\n10       ebt   include      funds\n11   program    family      stamp\n12 remaining      live       jobs\n13   reduced apartment     taking\n14  eligible   husband  graduated\n15      snap    school   planning\n\nthoughts_topics_15 &lt;- findThoughts(model = model_1,  California_raw_master$title_and_text,n=1, topics = 1)\n\n# Let's write a CSV so we can return to these top words later and do some qualitative analysis\nwrite_csv(topwords_1, \"topwords_15_topics\")"
  },
  {
    "objectID": "index.html#bar-chart",
    "href": "index.html#bar-chart",
    "title": "Topic Modeling & r/foodstamps",
    "section": "Bar chart",
    "text": "Bar chart\n\n# following this tutorial https://bookdown.enpchina.eu/PublicSphere_stm/PublicSphere_English.html#Topics_over_time\n\nday_20 &lt;- stm::estimateEffect(1:15 ~ day, model_1, meta=California_raw_master)\n\ntopicprop20&lt;-make.dt(model_1, meta)\ntopic20prop &lt;- topicprop20 %&gt;% select(c(2:16))\ntopic_proportion_per_year20 &lt;- aggregate(topic20prop, by = list(day = California_raw_master$day), mean)\n\nlibrary(reshape)\n\n\nAttaching package: 'reshape'\n\n\nThe following objects are masked from 'package:reshape2':\n\n    colsplit, melt, recast\n\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\nvizDataFrame20y &lt;- melt(topic_proportion_per_year20, id.vars = \"day\")\n\nheat_palette &lt;- heat.colors(20)\n\npalette &lt;- c(\"#2f4f4f\", \"#2e8b57\",\n             \"#800000\", \"#808000\",\"#000080\",\n             \"#ff0000\",\"#ff8c00\",\"#ffd700\",\"#00ff00\",\"#4169e1\",\"#00ffff\",\n             \"#00bfff\",\"#0000ff\",\"#da70d6\",\"#d8bfd8\") # Colors picked using a color palette generator\n\nmoneyshot &lt;- ggplot(vizDataFrame20y, aes(x=day, y=value, fill = variable)) + \n  geom_bar(stat = \"identity\") + ylab(\"proportion\") + \n  scale_fill_manual(values = palette, name = \"Topic\") + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))+\n  labs(title=\"topics over the month in r/foodstamps\", \n       subtitle = \"Topic proportion over time (posts that mention California)\") +\n  scale_x_continuous(breaks = c(15:31, 1:14), labels = c(15:31, 1:14))\n\nmoneyshot\n\n\n\nggsave(\"moneyshot_15_topics.png\", moneyshot, width = 6, height = 4, dpi = 300) # let's make sure to save the chart.\n\nThis chart is more helpful, but it gets hard to see all of the different topics. Still, it helps us to see which topics may change over the month. I got the idea for the facet wrap here ## All topics line chart\n\n# Line chart\nmoneyshot_1 &lt;- ggplot(vizDataFrame20y, aes(x=day,y=value, color = variable, group = variable)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Topics by day in r/foodstamps\",\n       subtitle= \"Posts that mention California\",\n       y = \"Expected Topic Proportion\",\n       x = \"Day of the month\") +\n  scale_x_continuous(breaks = c(1,5,10,15,20,25,30), labels = c(1,5,10,15,20,25,30)) +\n  facet_wrap(\"variable\") \n  \nmoneyshot_1 &lt;- moneyshot_1 + geom_vline(xintercept = 10, linetype = \"dashed\", color = \"red\") # Adding a dashed line at the 10th day of the month when SNAP money in California stops being disbursed\n\nmoneyshot_1\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nggsave(\"money_shot_1_15_topics.png\", moneyshot_1, width = 6, height = 4, dpi = 600)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nThe charts that we are most interested in allow us to zoom in on a particular topic. So let’s do that! ## Line charts of individual topics ### Topic 1 Zooming in on topic 1. We are filtering our chart from above to focus on topic 1.\n\ntopic_1 &lt;- vizDataFrame20y %&gt;% \n  filter(grepl(\"^Topic1$\", variable, ignore.case = TRUE)) \n\ntopic_1_chart_rdd &lt;- ggplot(topic_1, aes(x=day,y=value, color = variable, group = variable)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Topic 1 by day in r/foodstamps\",\n       subtitle= \"Posts that mention California\",\n       y = \"Expected Topic Proportion\",\n       x = \"Day of the month\") +\n  scale_x_continuous(breaks = c(1,5,10,15,20,25,30), labels = c(1,5,10,15,20,25,30)) +\n  geom_vline(xintercept = 10, linetype = \"dashed\", color = \"red\")\n\nprint(topic_1_chart_rdd)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\nTopic 2\n\ntopic_2 &lt;- vizDataFrame20y %&gt;% \n  filter(grepl(\"^Topic2$\", variable, ignore.case = TRUE)) \n\ntopic_2_chart_rdd &lt;- ggplot(topic_2, aes(x=day,y=value, color = variable, group = variable)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Topic 2 by day in r/foodstamps\",\n       subtitle= \"Posts that mention California\",\n       y = \"Expected Topic Proportion\",\n       x = \"Day of the month\") +\n  scale_x_continuous(breaks = c(1,5,10,15,20,25,30), labels = c(1,5,10,15,20,25,30)) +\n  geom_vline(xintercept = 10, linetype = \"dashed\", color = \"red\")\n\nprint(topic_2_chart_rdd)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nTopic 6\n\ntopic_6 &lt;- vizDataFrame20y %&gt;% \n  filter(grepl(\"^Topic6$\", variable, ignore.case = TRUE)) \n\ntopic_6_chart_rdd &lt;- ggplot(topic_6, aes(x=day,y=value, color = variable, group = variable)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Topic 6 by day in r/foodstamps\",\n       subtitle= \"Posts that mention California\",\n       y = \"Expected Topic Proportion\",\n       x = \"Day of the month\") +\n  scale_x_continuous(breaks = c(1,5,10,15,20,25,30), labels = c(1,5,10,15,20,25,30)) +\n  geom_vline(xintercept = 10, linetype = \"dashed\", color = \"red\")\n\nprint(topic_6_chart_rdd)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nTopic 9\n\ntopic_9 &lt;- vizDataFrame20y %&gt;% \n  filter(grepl(\"^Topic9$\", variable, ignore.case = TRUE)) \n\ntopic_9_chart_rdd &lt;- ggplot(topic_9, aes(x=day,y=value, color = variable, group = variable)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Topic 9 by day in r/foodstamps\",\n       subtitle= \"Posts that mention California\",\n       y = \"Expected Topic Proportion\",\n       x = \"Day of the month\") +\n  scale_x_continuous(breaks = c(1,5,10,15,20,25,30), labels = c(1,5,10,15,20,25,30)) +\n  geom_vline(xintercept = 10, linetype = \"dashed\", color = \"red\")\n\nprint(topic_9_chart_rdd)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nTopic 11\n\n# Selecting out topic 11\n\ntopic_11 &lt;- vizDataFrame20y %&gt;% \n  filter(grepl(\"^Topic11$\", variable, ignore.case = TRUE)) \n\ntopic_11_chart_rdd &lt;- ggplot(topic_11, aes(x=day,y=value, color = variable, group = variable)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Topic 11 by day in r/foodstamps\",\n       subtitle= \"Posts that mention California\",\n       y = \"Expected Topic Proportion\",\n       x = \"Day of the month\") +\n  scale_x_continuous(breaks = c(1,5,10,15,20,25,30), labels = c(1,5,10,15,20,25,30)) +\n  geom_vline(xintercept = 10, linetype = \"dashed\", color = \"red\")\n\nprint(topic_11_chart_rdd)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nWhile I could share the graphs for every topic, I think that these results are highly suggestive that there is some variation happening within the month that is influenced by external variables. For instance, perhaps the results of some topics are higher as the month draws to a close and rent comes due. Additionally, it is highly suggested by the charts that some topics are more prevalent during the first ten days of the month, suggesting the influence of the disbursal date on the topics within the subreddit.\nMoreover, rudimentary qualitative analysis of the different topics suggests that the topics are semantically coherent and suggestive of the different concerns that posters have. For instance, topic 9’s FREX words suggest concerns with verification, and topic 13’s suggest the concerns of students.\nRegression discontinuity design may be one way to move this project forward, as it allows for the development of statistical methods to measure how a variable changes over time in response to another variable. It could be a good fit in this case since we know how and when within each month the SNAP benefits are disbursed. This would allow for saying with certainty the precise way in which the topics change over time during each month.\nUltimately, additional work needs to be done to qualitatively analyze the posts within each topic to understand each topic’s potential semantic meaning. Moreover, interviews with caseworkers and policy analysts who participate in the forum would be helpful in understanding how they perceive their role in the forum, and would contribute to the development of tactical technical communication as a concept.\nMany businesses today, as well as technical communication as a discipline, are turning away from more formalized communications and towards user forums and documentation for issues. If it is indeed true that users are posting in r/foodstamps to receive assistance, it could be beneficial for states and government agencies to either participate in the forum themselves, or develop their own forums for users to post in."
  }
]